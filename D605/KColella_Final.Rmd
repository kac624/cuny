---
title: "CUNY SPS MSDS - DATA605 - Final"
author: "Keith Colella"
date: "`r Sys.Date()`"
output: html_document
---

```{r message = FALSE, warning = FALSE}
library(tidyverse)
```

# Problem 1 

*Using R, set a random seed equal to 1234 (i.e., set.seed(1234)). Generate a random variable X that has 10,000 continuous random uniform values between 5 and 15.Then generate a random variable Y that has 10,000 random normal values with a mean of 10 and a standard deviation of 2.89.*

```{r}
set.seed(1234)

X <- runif(n = 10^4, min = 5, max = 15)

Y <- rnorm(n = 10^4, mean = 10, sd = 2.89)

data.frame(X = X, Y = Y) %>%
  ggplot() +
  geom_histogram(aes(X), bins = 30, fill = 'red', alpha = 0.7) +
  geom_histogram(aes(Y), bins = 30, fill = 'blue', alpha = 0.7)
```

### Part 1

*Calculate as a minimum the below probabilities a through c. Assume the small letter "x" is estimated as the median of the X variable, and the small letter "y" is estimated as the median of the Y variable. Interpret the meaning of all probabilities.*

*a. P(X>x | X>y)*

We can approach this question both conceptually and empirically, given the distributions we generated above. First, let's set $x$ and $y$.

```{r}
x <- median(X)
y <- median(Y)

cat(
  'Median of X: ',x,'\n',
  'Median of Y: ',y,
  sep = ''
)
```

These values, both close to 10, conform to expectations. The median of both uniform and normal distributions are very close to the means, and we know the mean of both distributions will be ~10. For $X$, it's the halfway point between the min and max (5 and 15), and for $Y$, it's given.

Let's reframe our question with this in mind. We are looking for the probability that our random variable is greater than $x$, and we already know that our random variable is greater than $y$. Since both of those values are ~10, our probability should be very close to 1.

We can confirm this by creating a subset of cases where $X>y$, then calculating the proportion of those cases where $X$ is also less than $x$.

```{r}
B <- X[X>y]
A <- B[B>x]

proportion <- length(A) / length(B)

cat('P(A|B):', proportion)
```

Within our random sample, 100% of the values of $X$ within the subset $X>y$ are greater than $x$. In other words, $P(X>x | X>y) = 1.$

*b. P(X>x & Y>y)*

Whereas part (a) asked for a conditional probability, we're now looking at a joint probability. In other words, we're given nothing, and we have to find the probability that both of these conditions are met simultaneously.

Individually, the probability that a random variable is greater than the median of its respective distribution is 50%, since the median is, by definition, the 50th percentile. To be a bit more explicit, with a sample of 10,000, the probability would be

$$P(X > x) = \frac{(10,000/2)}{10,000} = \frac{5000}{10,000} = 0.5$$

And the same would apply to $P(Y > y)$. Moreover, we can estimate a joint probability $(PA \cap B)$ as the product of each individual probability: $P(A) \times P(B)$. So in this case, we would expect the probability to be 

$$P(X>x \cap Y>y) = P(X>x) \times P(Y>y) = 0.5 * 0.5 = 0.2499$$

Let's check this empirically. We can calculate the proportion of cases where $X>x$ compared to the total length of $X$, and the same for $Y>y$, then multiply the two.

```{r}
P_A <- length(X[X>x]) / length(X)
P_B <- length(Y[Y>y]) / length(Y)

P_AandB <- P_A * P_B

print(P_AandB)
```

It matches! Another way to check this empirically is to randomly sample from both distributions, then calculate the proportion of cases where both samples are greater than their respective medians.

```{r}
results <- data.frame()

for (n in 1:1000) {
  x_sample <- sample(X, 1)
  y_sample <- sample(Y, 1)
  result <- list(X = x_sample, Y = y_sample)
  results <- rbind(results, result)
}

proportion <- nrow(filter(results, X > x, Y > y)) / nrow(results)

print(proportion)
```

Our estimate of 0.25 holds true!

*c. P(X<x | X>y)*

We again have a condition probability, so we can employ a similar approach as our first problem. Moreover, given what we know about the first problem (i.e. that it's one), we can expect this probability to approach 0. Specifically, we know that $x$ and $y$ are both very close to 10. So if $X$ is greater than 10 (the given condition), then the chances of it also being less than 10 should be zero.

Let's check it from our distributions.

```{r}
B <- X[X>y]
A <- B[B<x]

proportion <- length(A) / length(B)

cat('P(A|B):', proportion)
```

Indeed, we find no cases where $X$ is both greater than $y$ and less than $x$, because both $x$ and $y$ are ~10.

### Part 2

*Investigate whether P(X>x & Y>y)=P(X>x)P(Y>y) by building a table and evaluating the marginal and joint probabilities.*

We used this definition of joint probability above. Let's now build a contigency table to substantiate the definition. We'll use conditional subsets to count the number of cases the meet the conditions of interest ($X>x$ and $Y>y$, as well as their complements ($X \leq x$ and $Y \leq y$). From there, we can divide by $n = 10,000$ to convert the table to show the proportion that each subset represents of the total sample, which gives us their probabilities.

```{r}
contingency <- table(
  factor(levels = c('X>x','X≤x','margin')), 
  factor(levels = c('Y>y','Y≤y','margin'))
)

contingency['X>x', 'margin'] <- length(X[X>x])
contingency['margin', 'Y>y'] <- length(Y[Y>y])

contingency['X≤x', 'margin'] <- length(X[X<=x])
contingency['margin', 'Y≤y'] <- length(Y[Y<=y])

contingency['X>x', 'Y>y'] <- length(X[(X>x) & (Y>y)])
contingency['X>x', 'Y≤y'] <- length(X[(X>x) & (Y<=y)])

contingency['X≤x', 'Y>y'] <- length(X[(X<=x) & (Y>y)])
contingency['X≤x', 'Y≤y'] <- length(X[(X<=x) & (Y<=y)])

print(contingency)

contingency_prop <- contingency/10^4

print(contingency_prop)
```

We can now see that the marginal probabilities $P(X>x)$ and $P(Y>y)$ both equal 0.5. This conforms to expectations, given that (as noted above), $x$ and $y$ represent the medians (or 50th percentiles) of their respective distributions. Moreover, we see that the joint probability $P(X>x \cap Y>y)$ is ~0.2507. Let's see if the product of $P(X>x)$ and $P(Y>y)$ comes out to the same.

```{r}
contingency_prop['X>x','margin'] * contingency_prop['margin', 'Y>y']
```

Indeed, it matches. Our estimated probability in the contigency table doesn't *exactly* equal 0.25 because our random sampling is, well, random!

### Part 3

*Check to see if independence holds by using Fisher’s Exact Test and the Chi Square Test. What is the difference between the two? Which is most appropriate? Are you surprised at the results? Why or why not?*

Both of these tests examine whether there are nonrandom associations between two variables. In the absence of such associations, we can conclude that the variables are independent. Both tests take as the null hypothesis ($H_0$) that the variables are independent, and the alternative hypothesis ($H_1$) is that they are not. Both produce a p-value that, as it approaches 0, gives us more confidence in rejecting alternative hypothesis. Finally, both require a contigency table as an input.

In terms of differences, one key difference is related to sample size. Whereas the Chi-Squared test assumes that samples are random and representative, and that there are at least ~5 observations in each square of the contigency table, the Fisher Exact Test is typically used for smaller sample sizes (although it technically still applies to larger samples, too). Moreover, the Chi-Squared test relies upon the chi-squared distribution (which becomes more accurate as sample size increases), whereas the Fisher Test relies upon no parametric distribution. Finally, the Fisher test is a bit more computationally intensive (especially with large sample sizes).

So, in terms of applicability, the Chi-Squared test is typically more appropriate for large sample sizes, while Fisher Test is often used with smaller samples. For situation, we have a very large sample of 10,000, so the Chi-Squared test seems most appropriate. Lucky for us, R has implementations for both of these tests, so let's check both!

```{r}
p_chi <- chisq.test(contingency[1:2, 1:2])$p.value

p_fish <- fisher.test(contingency[1:2, 1:2])$p.value

cat(
  'Chi-Squared Test Result:', p_chi, '\n',
  'Fisher Exact Test Result:', p_fish,
  sep = ''
)
```

In both cases, our p-value is well above zero. So, with just about any reasonable significance level / $\alpha$, we do not reject the null that the two variables are independent. Thus, we can conclude they *are* independent.

Given that we generated these variables with entirely independent processes (`rnorm` and `runif`), it is unsurprising that these tests helped us conclude that they are independent. Moreover, they come from very different different distributions, so again, one would expect independence.

# Problem 2

*You are to register for Kaggle.com (free) and compete in the Regression with a Crab Age Dataset competition: https://www.kaggle.com/competitions/playground-series-s3e16.*

```{r}
train <- read.csv('data/crab_train.csv')
test <- read.csv('data/crab_test.csv')
```

### Descriptive and Inferential Statistics

*Provide univariate descriptive statistics and appropriate plots for the training data set.*

```{r}
for (col in colnames(train)){
  if(col == 'id' | col == 'Sex') next
  cat('-----',col,'-----\n')
  print(summary(train[[col]]))
  hist(train[[col]], main = col, xlab = col)
}
```

```{r}
barplot(table(train$Sex), main = "Sex", xlab = "Sex", ylab = "Frequency")
```

Across nearly all of our variables, we see quite a bit of skew. Specifically, Length and Diameter exhibit left skew, whereas Height, Age and the various Weight measures exhibit right skew. When modeling, these variables may benefit from transformation. Sex, on the other hand, is relatively well balanced across the three classes.

*Provide a scatterplot matrix for at least two of the independent variables and the dependent variable.* 

```{r}
pairs(~ Height + Weight + Length + Age, data = train)
```

We see some evidence of linear relationships between these three primary predictors and Age. The relationships don't appear perfectly linear, but they seem sufficient for explore in modeling. Moreover, some transformation may reinforce the linearity of the relationships.

*Derive a correlation matrix for any three quantitative variables in the dataset.*

```{r}
cor(train[, c('Height', 'Weight', 'Length')])
```

We see high correlation across these primary independent variables. This conforms to expectations, as all three relate to size. We'll need to consider whether this multicollinearity could impact our modeling.

*Test the hypotheses that the correlations between each pairwise set of variables is 0 and provide an 80% confidence interval. Discuss the meaning of your analysis. Would you be worried about familywise error? Why or why not?*

```{r}
variables <- colnames(train)[-c(1,2)]
variables <- c(variables[length(variables)], variables[-length(variables)])
combos <- combn(variables, 2, simplify = FALSE)

results <- data.frame()
for (combo in combos) {
  cor_test <- cor.test(train[[combo[1]]], train[[combo[2]]], conf.level = 0.80)
  result <- list(
    Var1 = combo[1], Var2 = combo[2], p = cor_test$p.value,
    ci_lower = cor_test$conf.int[1], ci_lower = cor_test$conf.int[2]
  )
  results <- rbind(results, result)
}

results
```

Our testing indicates that **all** pairs of variables have correlations greater than 0. I'll note that the p-values from these tests are not actually zero. Instead, they are exceedingly small and approach zero; the `cor.test` function simply rounds it to zero because it's so small. Still, they all indicate that we should reject the null that correlations are zero, instead concluding they are non-zero.

While this is a "good" thing for variables' relationship with Age (since it indicates they are likely good predictors), it also indicates that we'll need to account for multicollinearity in modeling.

Because we are conducting a series of statistically tests concurrently, we should consider familywise error rate, which is the probability of false positives in our findings. With such a large series of tests (28 in total), the methods for adjusting our testing (e.g. Bonferonni correction) may drive overly conservative alphas. However, our p-values are already near zero, so we can apply a correction anyway and see if it impacts any conclusions.

The Bonferonni correction requires us to adjust our $\alpha$ by dividing by on the number of tests conducted (28). We can then subtract this adjusted $\alpha$ from 1 to get our new confidence level.

```{r}
results <- data.frame()
for (combo in combos) {
  cor_test <- cor.test(train[[combo[1]]], train[[combo[2]]], conf.level = 1 - (0.20 / 28))
  result <- list(
    Var1 = combo[1], Var2 = combo[2], p = cor_test$p.value,
    ci_lower = cor_test$conf.int[1], ci_lower = cor_test$conf.int[2]
  )
  results <- rbind(results, result)
}

results
```

Even with the correction, our conclusion remains the same — all pairs of variables have non-zero correlation. Our confidence intervals are slightly wider, but in all cases, the correlation appears quite strong (>0.9) for pairs of predictor variables. When paired with our target variable (Age), predictor variables have slightly lower levels of correlation (~0.5 to ~0.7), but still enough to indicate that they may be good predictors.

### Linear Algebra and Correlation

*Invert your correlation matrix from above. (This is known as the precision matrix and contains variance inflation factors on the diagonal.) Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix. Conduct LDU decomposition on the matrix.*

I didn't have an explicit correlation matrix with all variables, so I'll first generate that. Note that I'll remove the ID column (since it's not actually a numerical value, just a unique key), and the Sex column (since its not numerical).

```{r}
cor_matrix <- cor(select(train, -id, -Sex))
cor_matrix
```

Next, we can compute the precision matrix by inverting this correlation matrix. We'll use the `inv` function from `pracma`.

```{r}
precision_matrix <- pracma::inv(cor_matrix)
precision_matrix
```

Finally, we can multiply these two and perform our LDU decomposition. We'll start by calculating the L and U components with `lu.decomposition` function from `matrixcalc`. We can then pull out the diagonals from the U component to form our D component. With that, we need to then adjust our U component and scale its diagonal down to 1 by dividing by the diagonal of D.

```{r}
product_matrix <- cor_matrix %*% precision_matrix

lu_decomp <- matrixcalc::lu.decomposition(product_matrix)

L <- lu_decomp$L
U <- lu_decomp$U
D <- diag(diag(U))
U <- U / diag(D)

cat('L component:\n')
print(L)
cat('D component:\n')
print(D)
cat('U component:\n')
print(U)
```

We can check our decomposition by multiplying these three component matrixes and confirming that their product matches the original `product_matrix`. 

```{r}
reformed_matrix <- L %*% D %*% U

cat('Original Product Matrix:\n')
print(matrix(scales::scientific(product_matrix), nrow = 8))
cat('Reformed Matrix from LDU:\n')
print(matrix(scales::scientific(reformed_matrix), nrow = 8))
cat('Match Check:',all.equal(round(matrix(product_matrix,nrow = 8),20),round(reformed_matrix,20)))
```

Our reconstruction was successful! Note that the value in `[7,8]` is actually zero in both matrices, but appears as -3.50e-46 in the reconstruction due to floating point precision.

One thing to note here is that our initial `product_matrix` is symmetrical and has unit diagonals (i.e. 1s along the diagonal). As a result, the U component is simply the transpose of L. Moreover, the D component is simply an identity matrix. We can demonstrate this with an alternative deconstruction.

```{r}
lu_decomp <- matrixcalc::lu.decomposition(product_matrix)

L_alt <- lu_decomp$L
U_alt <- t(L_alt)
D_alt <- diag(8)

cat('L component:\n')
print(L_alt)
cat('D component:\n')
print(U_alt)
cat('U component:\n')
print(D_alt)

reformed_matrix_alt <- L %*% D %*% U

cat('Original Product Matrix:\n')
print(matrix(scales::scientific(product_matrix), nrow = 8))
cat('Reformed Matrix from LDU:\n')
print(matrix(scales::scientific(reformed_matrix_alt), nrow = 8))
cat('Match Check:',all.equal(round(matrix(product_matrix,nrow = 8),20),round(reformed_matrix_alt,20)))
```



### Calculus-Based Probability & Statistics

*Many times, it makes sense to fit a closed form distribution to data. Select a variable in the Kaggle.com training dataset that is skewed to the right, shift it so that the minimum value is absolutely above zero if necessary. Then load the MASS package and run fitdistr to fit an exponential probability density function. Find the optimal value of $\lambda$ for this distribution, and then take 1000 samples from this exponential distribution using this value (e.g., rexp(1000, $\lambda$)). Plot a histogram and compare it with a histogram of your original variable.  Using the exponential pdf, find the 5th and 95th percentiles using the cumulative distribution function (CDF).  Also generate a 95% confidence interval from the empirical data, assuming normality. Finally, provide the empirical 5th percentile and 95th percentile of the data. Discuss.*

We'll use the `Weight` variable for this exercise, as it demonstrates right skew.

```{r}
hist(train$Weight, main = 'Weight', xlab = 'Weight')
```

A shift is not needed here, as the minimum value for Weight is already greater than 0.

```{r}
min(train$Weight)
```

Now, we can use the `fitdistr` function from `MASS` to fit an exponential distribution. This function provides an estimate for the optimal parameter of whichever distribution you choose, which this case is $\lambda$ for an exponential. 

```{r}
weight_exp <- MASS::fitdistr(train$Weight, "exponential")
cat('Estimated Optimal Lambda:', weight_exp$estimate)
```

Let's sample from this distribution and compare the resulting plot to the original variable.

```{r}
weight_exp_sample <- rexp(1000, weight_exp$estimate)

hist(weight_exp_sample, main = 'Simulated Distribution of Weight from Exponential', xlab = 'Weight')
hist(train$Weight, main = 'Original Distribution of Weight', xlab = 'Weight')
```

Our simulated distribution appears to be very rough approximation of the actual `Weight` variable. It exhibits the same right skew, as expected, but it also produced much greater tail values. The max in our distribution is greater than 150, but the true max weight is closer to 80.

Let's compare the 5th and 95th quantiles from each distribution, along with a 95% confidence interval. For the confidence  interval, we'll use the following formula:

$$\bar{x} \pm (\frac{s}{\sqrt{n}} \times zscore)$$

The mean and standard deviation of our sample will serve as $\bar{x}$ and $s$, respectively; our sample size will serve as $n$; and we can generate a z-score with the `qnorm` function (which comes out to ~1.96 for a 95% CI).

```{r}
exp_5th <- qexp(0.05, rate = weight_exp$estimate)
exp_95th <- qexp(0.95, rate = weight_exp$estimate)

emp_5th <- quantile(train$Weight, 0.05)
emp_95th <- quantile(train$Weight, 0.95)

ci <- mean(train$Weight) + c(-1, 1) * sd(train$Weight) / sqrt(length(train$Weight)) * qnorm(0.975)

cat(
  'Exponential 5th percentile:', exp_5th,
  '\nExponential 95th percentile:', exp_95th,
  '\nEmpirical 5th percentile:', emp_5th,
  '\nEmpirical 95th percentile:', emp_95th,
  '\n95% CI:', ci,
  '\nSimulated distribution mean:',mean(weight_exp_sample)
)
```

As with the plots above, we see significant differences between our empirical and parametric distributions. The 5th and 95th percentiles of the parametric distribution are lower and higher than the observed 5th and 95th percentiles. The mean of our simulated distribution also sits outside of the 95% confidence interval for the true population mean of `Weight` (although it is quite close).

### Modeling

*Build some type of multiple regression model and submit your model to the competition board. Provide your complete model summary and results with analysis. Report your Kaggle.com user name and score.*

##### Baseline

Let's start with an initial linear fit to serve as a baseline.

```{r}
lm_base <- lm(Age ~ ., data = select(train, -id))
summary(lm_base)
plot(lm_base)
```

These plots highlight some potential issues. The first and third plots indicate heteroskedasticity. We noted before significant skew in most of our variables, which is likely driving inconsistent variance in residuals. We'll consider some transformations in our next iteration of the model.

The fourth indicates two very influential points at indexes 55880 and 19024. If we take a look at these points, nothing appears especially significant. All predictor variables are roughly around the first quartile in terms of their respective distributions. The exceptions are Shucked and Viscera Weight, which are relatively high compared to overall Weight. So, if anything, this observation seems for be for a crab that has relatively high Shucked and Viscera Weight. I don't think this warrants removal at these points, but we'll keep an eye out as we adjust the model.

```{r}
train[c('55880','19024'), ]
```

Previously, we had also identified high correlation among predictors. Let's check the Variance Infaltion Factor to see if the model exhibits high multicollinearity.

```{r}
car::vif(lm_base)
```

Looking at the adjusted VIF (the third column). It seems there is some significant multicollinearity with Length, Diameter and Weight. If this condition persists after transformation, I'll consider feature selection techniques to remove correlated pairs.

##### Power Transformation

I'll use the `powerTransform` function from `car` to address the skew in our variables.

```{r}
train_transform <- train

train_transform[['Height']] <- train_transform[['Height']] + 1

for (predictor in colnames(select(train_transform, -id, -Sex, -Age))) {
  formula <- as.formula(paste(predictor,' ~ 1'))
  transformation <- car::powerTransform(formula, data = train_transform)
  transformed <- car::bcPower(train_transform[predictor], transformation$lambda)
  train_transform[predictor] <- transformed
}

transformation <- car::powerTransform(Age ~ ., data = train_transform)
transformed <- car::bcPower(train_transform['Age'], transformation$lambda)
train_transform['Age'] <- transformed
```

Let's re-examine these variables now that they're transformed.

```{r}
histograms <- function(df) {
 plots <- list()
 
 for (i in 1:ncol(df)) {
 col <- colnames(df)[i] 
 p <- df %>%
 ggplot(aes(!!sym(col))) +
 geom_histogram(bins = 20)
 plots[[i]] <- p
 }
 
 return(cowplot::plot_grid(plotlist = plots, nrow = 3))
}

histograms(select(train_transform, -id, -Sex))
pairs(select(train_transform, -id, -Sex))
```

Our distributions appear much more normal, and the linear relationships are much clearer. Multicollinearity persists, but we'll address that shortly.
 
For now, let's refit the model.

```{r}
lm_pt <- lm(Age ~ ., data = select(train_transform, -id))
summary(lm_pt)
plot(lm_pt)
```

We see some of the original issues persist. Given the persistence of these issues, along with multicollinearity, I believe Principal Component Analysis (PCA) may be a viable option. Let's give it a go!

##### PCA

```{r}
pca_inputs <- scale(select(train, -id, -Age, -Sex))
pca_fit <- prcomp(pca_inputs, center = TRUE, scale. = TRUE)

summary(pca_fit)
plot(pca_fit)
```

We end up with 7 principal components, although the first drives explains the vast majority of the variance. This is somewhat expected, given the high degree of multicollieanrity and redundancy across predictors. I'll therefore only take the first three components to use in modeling.

We'll add back the categorical Sex variable, along with our target variable, then fit another model.

```{r}
train_pca <- data.frame(pca_fit$x[, 1:3]) %>%
  mutate(Sex = train$Sex, Age = train$Age)

lm_pca <- lm(Age ~ ., data = train_pca)
summary(lm_pca)
plot(lm_pca)
```

Our residual plots look  much better. However, our R-squared is relatively low at ~50%, and the two influential points remains. Let's fit this model without that point and see how things change.

```{r}
train_pca2 <- train_pca[-which(rownames(train_pca) == '55880'),]
train_pca2 <- train_pca2[-which(rownames(train_pca2) == '19024'),]

lm_pca <- lm(Age ~ ., data = train_pca2)
summary(lm_pca)
plot(lm_pca)
```

I don't actually see a big difference in coefficients, so it appears we are fine to leave out these points.

Ultimately, it seems there is some non-linearity in the model. So, modeling approaches that handle non-linearity may provide better fits. Regarding the linear models we've considered, however, this appears to provide the best while observing the key assumptions of regression.

With this final model, we can generate our predictions for submission on Kaggle!

```{r}
test_scaled <- scale(select(test, -id, -Sex))
test_pca <- predict(pca_fit, newdata = test_scaled)
test_pca<- data.frame(test_pca[, 1:3]) %>%
  mutate(Sex = test$Sex)
predictions <- predict(lm_pca, newdata = test_pca)
predictions <- data.frame(id = test$id, Age = predictions)

write.csv(predictions, 'data/crab_kaggle_submission_kac624.csv', row.names = FALSE)
```

My submission had a score (the MAE) of 1.57413. Not bad! Curious to see how this stacks up to other submissions.
