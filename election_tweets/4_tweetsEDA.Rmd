---
title: "tweetsEDA"
author: "Keith Colella"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, message = FALSE}
library(tidyverse)
library(httr)
library(tidytext)
library(kableExtra)
library(superml)
library(data.table)

# library(wordcloud)
# library(lexicon)
```

## Data

```{r}
kaggle <- jsonlite::read_json('data/kaggle.json')
username <- kaggle$username
authkey <- kaggle$key

url <- paste0('https://www.kaggle.com/api/v1/datasets/download/',
              'kac624/politicaltweets/candidate_tweets2022_04.29.csv')
response <- GET(url, authenticate(username, authkey, type = 'basic'))
temp <- tempfile()
download.file(response$url, temp, mode = 'wb')
tweets <- read_csv(unz(temp, 'candidate_tweets2022_04.29.csv'))
unlink(temp)
rm(response)
```

Clean up

```{r}
tweets <- tweets %>%
  select(-...1) %>%
  filter(year(date_created) > 2020,
         year(date_created) < 2023,
         is.na(retweeted_id),
         text != '')

tweets$text <- tweets$text %>%
  str_remove_all('&amp;') %>%
  str_remove_all('https://t.co/[a-z,A-Z,0-9]*') %>%
  str_remove_all('[â¢\u0080-\uFFFF]') %>%
  str_remove_all('\\p{So}') %>%
  str_remove_all('\'s') %>%
  str_replace_all('[[:cntrl:]]', ' ')

tweets <- tweets[!duplicated(tweets$tweet_id), ]

tweets %>%
  select(username, text) %>%
  head(10) %>%
  kable(align = 'l') %>% 
  kable_classic(position = 'center')
```

Add in candidate data

```{r}
candidates <- read_csv('data/candidates2022_clean.csv')

tweets <- tweets %>%
  mutate(username = tolower(username)) %>%
  left_join(
    candidates %>%
      mutate(twitter_name = tolower(twitter_name)) %>%
      select(name, state, district, party_simple, twitter_name),
    by = c('username' = 'twitter_name'), keep = FALSE
  )

tweets <- tweets[!duplicated(tweets), ]

tweets <- tweets %>%
  filter(!(name == 'REYNOLDS, CONRAD EARL'))
```

EDA / Visualizations

```{r}
tweets %>%
  ggplot(aes(party_simple)) +
  geom_bar() +
  scale_y_continuous(labels = scales::comma)

tweets %>%
  group_by(name) %>%
  summarize(tweet_count = n(), .groups = 'keep') %>%
  ggplot(aes(tweet_count)) +
  geom_histogram() +
  scale_y_continuous(labels = scales::comma)

tweets %>%
  group_by(month = floor_date(date_created, 'month')) %>%
  summarize(count = n()) %>%
  ggplot(aes(month, count)) +
  geom_col() +
  scale_y_continuous(labels = scales::comma) +
  coord_flip()

tweets %>%
  group_by(state) %>% 
	summarise(count = n()) %>% 
  ggplot(aes(x = reorder(state, count), y = count)) +
  geom_bar(stat = 'identity') +
  scale_x_discrete(guide = guide_axis(n.dodge=2)) +
  coord_flip()

tweets %>%
  filter(replies < 100) %>%
  ggplot(aes(replies)) +
  geom_histogram(bins = 50) +
  scale_x_continuous(labels = scales::comma) +
  scale_y_continuous(labels = scales::comma)

tweets %>%
  filter(likes < 1000) %>%
  ggplot(aes(likes)) +
  geom_histogram(bins = 50) +
  scale_x_continuous(labels = scales::comma) +
  scale_y_continuous(labels = scales::comma)
```

Next?

```{r}
# tweets_tf_idf <- tweets %>%
#   group_by(name) %>%
#   mutate(tweet_count = n()) %>%
#   ungroup() %>%
#   filter(tweet_count > 100) %>%
#   unnest_tokens(word, text) %>%
#   count(name, word, sort = TRUE) %>%
#   bind_tf_idf(word, name, n)
# 
# tweets_tf_idf %>%
#   arrange(desc(tf_idf))
```

Calculating frequencies

```{r}
# tweets_consolidated <- tweets %>%
#   filter(party_simple != 'Other') %>%
#   group_by(name, party_simple) %>%
#   summarize(text = paste0(text, collapse = ''), .groups = 'keep') %>%
#   mutate(party_simple = if_else(party_simple == 'Republican', 1, 0))
  
data('stop_words')

tweets_consolidated <- tweets %>%
  group_by(name) %>%
  mutate(tweet_count = n()) %>%
  ungroup() %>%
  filter(tweet_count > 100,
         party_simple != 'Other') %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words, by = c('word')) %>%
  group_by(name, party_simple) %>%
  summarize(text = paste0(word, collapse = ' '), .groups = 'keep') %>%
  mutate(party_simple = if_else(party_simple == 'Republican', 1, 0),
         hashtags = str_extract_all(text, '#[a-zA-Z0-9]+'),
         hashtags = map_chr(hashtags, str_c, collapse = ' '))

sample <- sample(c(rep(0, round(0.8 * nrow(tweets_consolidated))), 
                   rep(1, round(0.2 * nrow(tweets_consolidated)))))

train <- tweets_consolidated[sample == 0, ] %>%
  data.table() %>% select(-name)
test <- tweets_consolidated[sample == 1, ] %>%
  data.table() %>% select(-name)

head(train, 3)
head(test, 3)
```

Fit tf-idf based on text

```{r}
tfv_text <- TfIdfVectorizer$new(min_df = 0.3, max_features = 30, remove_stopwords = F)

tfv_text$fit(train$text)
features_train_text <- tfv_text$transform(train$text)
features_test_text <- tfv_text$transform(test$text)

dim(features_train_text)
dim(features_test_text)

x_train_text <- data.table(features_train_text, party = train$party_simple)
x_test_text <- data.table(features_test_text, party = test$party_simple)
```

Fit tf-idf based on hashtags

```{r}
tfv_hash <- TfIdfVectorizer$new(# min_df = 0.3, max_features = 30)
  min_df = 0.5, max_features = 20)

tfv_hash$fit(train$hashtags)
features_train_hash <- tfv_hash$transform(train$hashtags)
features_test_hash <- tfv_hash$transform(test$hashtags)

dim(features_train_hash)
dim(features_test_hash)

x_train_hash <- data.table(features_train_hash, party = train$party_simple)
x_test_hash <- data.table(features_test_hash, party = test$party_simple)
```

## Models

```{r}
x_train <- x_train_text
x_test <- x_test_text
```

Logistic Regression

```{r}
lf <- LMTrainer$new(family = 'binomial')
lf$fit(X = x_train, y = 'party')
summary(lf$model)

lf_pred <- lf$predict(x_test)
Metrics::auc(actual = x_test$party, predicted = lf_pred)
caret::confusionMatrix(as.factor(as.numeric(lf_pred > 0.5)), 
                       as.factor(x_test$party))

full_data <- rbind(x_train, x_test)
lf_pred_full <- lf$predict(full_data)
Metrics::auc(actual = full_data$party, predicted = lf_pred_full)
caret::confusionMatrix(as.factor(as.numeric(lf_pred_full > 0.5)), 
                       as.factor(full_data$party))

data.frame(pred = lf_pred_full) %>%
  ggplot(aes(pred)) +
  geom_histogram(bins = 40)
```

COMBO

```{r}
x_train <- cbind(x_train_text, 
                 x_train_hash %>%
                   select(-party, -vote, -any_of(bad_cols)))
x_test <- cbind(x_test_text,
                x_test_hash %>%
                   select(-party, -vote, -any_of(bad_cols)))

#####

# bad_cols <- unlist(str_extract_all(colnames(x_train_hash), '[a-z]{2}[0-9]{2}'))
# x_train_hash_2 <- x_train_hash %>%
#   select(-vote)
# 
# plotAllLayers <- function(df) {
#   p <- ggplot(data = df)
#   for(col in names(df)[-31]){ 
#     p <- p + geom_density(aes_string(col))
#   }
#   return(p)
# }
# plotAllLayers(x_train_text)
# plotAllLayers(x_train_hash_2)
# 
# min(unlist(lapply(x_train_hash, mean)))

#####

lf <- LMTrainer$new(family = 'binomial')
lf$fit(X = x_train, y = 'party')
summary(lf$model)

lf_pred <- lf$predict(x_test)
Metrics::auc(actual = x_test$party, predicted = lf_pred)
caret::confusionMatrix(as.factor(as.numeric(lf_pred > 0.5)), 
                       as.factor(x_test$party))

full_data <- rbind(x_train, x_test)
lf_pred_full <- lf$predict(full_data)
Metrics::auc(actual = full_data$party, predicted = lf_pred_full)
caret::confusionMatrix(as.factor(as.numeric(lf_pred_full > 0.5)), 
                       as.factor(full_data$party))

data.frame(pred = lf_pred_full) %>%
  ggplot(aes(pred)) +
  geom_histogram(bins = 40)
```

Naive-Bayes

```{r}
nb <- NBTrainer$new()
nb$fit(x_train, 'party')
pred <- nb$predict(x_test)
Metrics::auc(actual = x_test$party, predicted = pred)
caret::confusionMatrix(pred, as.factor(x_test$party))
```

XGBoost

```{r}

```

Export

```{r}
write_csv(x_train, 'data/tf-idf_train.csv')
write_csv(x_train, 'data/tf-idf_test.csv')
```

