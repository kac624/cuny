---
title: "Data 607 - Week 9 - Web APIs"
author: "Keith Colella"
date: "`r Sys.Date()`"
output: html_document
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = "../D607/output"
    )
  })
---

## Assignment

Your task is to choose one of the New York Times APIs, construct an interface in R to read in the JSON data, and transform it into an R DataFrame.

## Setup

In addition to standard `tidyverse` usage, we'll leverage the `jsonlite` for querying the API, `lubridate` for formatting dates, and `kableExtra` to display our results.

```{r setup, message = FALSE}
library(tidyverse)
library(jsonlite)
library(lubridate)
library(kableExtra)
```

For this assignment, I've focused on the New York Time's Article Search API. The API instructions note that equests will be limited to 10 articles unless query hits are spaced at least 6 seconds apart. For the sake of this exercise, we'll ignore timing and work within the 10-article limit.

The API requires a key, which users can obtain by creating an account at https://developer.nytimes.com/. I'll read in my key from a .txt file.

```{r}
key <- read_lines('data/nyt_api_key.txt')
```

We'll begin with a static example. Per the instructions on the NYT API page, we'll use the base API url and add key terms to filter our search. For now, we'll just use a simple keyword search (`q=`) within a specific date range (using `&begin_date=` and `&end_date=`).

```{r}
base_url <- 'https://api.nytimes.com/svc/search/v2/articlesearch.json?'
search_term <- 'gerrymandering'
begin_date <- '20230101'
end_date <- '20230331'

url <- str_c(base_url,
             'q=',search_term,
             '&begin_date=',begin_date,
             '&end_date=',end_date,
             '&api-key=',key)

request <- fromJSON(url)
```

The JSON we obtain from our request is a large, heavily nested list. After some exploration, it becomes clear that the actual content of the query is in the `response$docs$` sublist. I create a variable of that sublist, then parse out key pieces of content from within that sublist into a dataframe. For now, I've focused on key article-related details (e.g. date, author, desk) along with whatever actual text content is available (headline, abstract and lead paragraph). 

```{r}
response <- request$response$docs

df <- data.frame(pub_date = as_date(response$pub_date),
                 byline = response$byline$original,
                 news_desk = response$news_desk,
                 document_type = response$document_type,
                 type_of_material = response$type_of_material,
                 word_count = response$word_count,
                 headline = if_else(!is.na(response$headline$print_headline),
                                    response$headline$print_headline,
                                    response$headline$main),
                 abstract = response$abstract,
                 lead_paragraph = response$lead_paragraph
                 )
```

We can preview the first three rows of the resulting dataframe to get a sense of our results.

```{r}
df[1:3,] %>% kbl() %>% kable_classic()
```

Now that we've tested our approach, we can create a function to enable more efficient programming of queries. I've added some functionality to (i) add additional search parameters, and (ii) allow certain search parameters to be optional.

```{r}
nyt_query <- function(search_term, begin_date = NULL, 
                      end_date = NULL, news_desk = NULL, 
                      type_of_material = NULL) {
  url <- str_c(base_url,
               'q=',search_term,
               if_else(is.null(begin_date), '',
                       str_c('&begin_date=',begin_date)),
               if_else(is.null(end_date), '',
                       str_c('&end_date=',end_date)),
               if_else(is.null(news_desk), '',
                       str_c('&fq=news_desk:(',news_desk,')')),
               if_else(is.null(type_of_material), '',
                       str_c('&fq=type_of_material:',type_of_material)),
               '&api-key=',key)
  
  request <- fromJSON(url)
  
  response <- request$response$docs
  
  df <- data.frame(pub_date = as_date(response$pub_date),
                   byline = response$byline$original,
                   news_desk = response$news_desk,
                   document_type = response$document_type,
                   type_of_material = response$type_of_material,
                   word_count = response$word_count,
                   headline = if_else(!is.na(response$headline$print_headline),
                                      response$headline$print_headline,
                                      response$headline$main),
                   abstract = response$abstract,
                   lead_paragraph = response$lead_paragraph
  )
  
  return(df)
}
```

We first test this by re-producing our original query to confirm we obtain comparable results.

```{r}
nyt_query(search_term = 'gerrymandering', 
          begin_date = '20230101')[1:3,] %>% 
  kbl() %>% kable_classic()
```

Finally, we can try a few additional queries with different parameters.

```{r}
nyt_query(search_term = 'svb', 
          begin_date = '20230301',
          end_date = '20230320',
          type_of_material = 'News') %>% 
  kbl() %>% kable_classic()

nyt_query(search_term = 'svb', 
          begin_date = '20230301',
          end_date = '20230320',
          news_desk = 'Business',
          type_of_material = 'News') %>% 
  kbl() %>% kable_classic()

nyt_query(search_term = 'svb', 
          begin_date = '20230301',
          end_date = '20230320',
          type_of_material = 'Op-Ed') %>% 
  kbl() %>% kable_classic()
```


