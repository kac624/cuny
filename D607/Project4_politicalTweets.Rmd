---
title: "Project4"
author: "Keith Colella"
date: "`r Sys.Date()`"
output: html_document
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = "../D607/output"
    )
  })
---

```{r setup, message = FALSE}
library(tidyverse)
library(httr)
library(jsonlite)
library(fuzzyjoin)
```

## Initial Universe of Candidates

https://api.open.fec.gov/developers/
https://github.com/stephenholzman/tidyusafec

```{r}
year = 2022
fec_key <- read_lines('data/fec_api.txt')

query_params <- list(election_year = year,
                     office = 'H',
                     api_key = fec_key)
url <- modify_url('https://api.open.fec.gov/', 
                  path = '/v1/candidates/search/',
                  query = query_params)

initial_response <- GET(url)
parsed_response <- fromJSON(content(initial_response, "text", encoding="UTF-8"), 
                            simplifyVector = FALSE)
total_pages <- parsed_response$pagination$pages
```

Loop through pages to get total responses

```{r}
response <- list()

for (i in 1:total_pages) {
  query_params <- list(page = i,
                       election_year = year,
                       office = 'H',
                       api_key = fec_key)
  url <- modify_url('https://api.open.fec.gov/', 
                    path = '/v1/candidates/search/',
                    query = query_params)
  initial_response <- GET(url)
  parsed_response <- fromJSON(content(initial_response, 'text', encoding='UTF-8'), 
                              simplifyVector = FALSE)
  response[[i]] <- parsed_response
  Sys.sleep(0.5)
}
```

Loop through responses and grab individual observations

```{r}
candidates <- map(response, function(x) x$results) %>%
    unlist(recursive = F) %>%
    tibble(
      fec_id = map_chr(., 'candidate_id', .default = NA),
      name = map_chr(., 'name', .default = NA),
      state = map_chr(., 'state', .default = NA),
      district = map_chr(., 'district', .default = NA),
      party = map_chr(., 'party_full', .default = NA),
      office = map_chr(., 'office_full', .default = NA),
      incumbent_challenge = map_chr(., 'incumbent_challenge_full', .default = NA),
      candidate_status = map_chr(., 'candidate_status', .default = NA),
    ) %>%
  select(-.)

candidates %>% head()

og_candidates <- candidates
```

## Twitter IDs for candidates

https://ballotpedia.org/List_of_congressional_candidates_in_the_2022_elections

```{r, message = FALSE}
ballotpedia <- read_csv('data/ballotpedia_scrape2022.csv')

ballotpedia <- ballotpedia[!duplicated(ballotpedia),] %>%
  filter(twitter != '[]') %>%
  mutate(district = if_else(is.na(district), '00', sprintf("%02d", district)),
         state = str_replace_all(state, '_', ' '),
         twitter_name = str_extract_all(twitter, '(?<=twitter.com/)\\w+(?=\')'))

ballotpedia

og_ballotpedia <- ballotpedia
```

https://www.rdocumentation.org/packages/stringdist/versions/0.9.10/topics/stringdist-metrics

```{r}
candidates <- og_candidates
ballotpedia <- og_ballotpedia

candidates <- candidates %>%
  mutate(party_simple = case_when(party == 'REPUBLICAN PARTY' ~ 'Republican',
                                  party == 'DEMOCRATIC PARTY' ~ 'Democrat',
                                  TRUE ~ 'Other'),
         key = case_when(!str_detect(name,',') ~ name,
                          TRUE ~ str_c(str_extract(name, '(?<=, ).*'), ' ',
                                       str_extract(name, '.*(?=,)'))) %>%
           str_remove_all(str_c(
             '(MS\\.|MRS\\.|MR\\.|DR\\.|JR\\.|JR|SR\\.|III|II|',
             '\\b[A-Z]\\.\\b|\\b[A-Z]\\b|É|[0-9])')) %>%
           str_remove_all('[[:punct:]]') %>%
           str_replace_all('  ', ' ') %>%
           trimws() %>%
           tolower(),
         key = str_c(str_extract(key, '^[A-Za-z]+'), ' ',
                 str_extract(key, ' [A-Za-z]+$')))

ballotpedia <- ballotpedia %>%
  mutate(state = state.abb[match(state,state.name)],
         party_simple = case_when(party == 'Republican Party' ~ 'Republican',
                                  party == 'Democratic Party' ~ 'Democrat',
                                  TRUE ~ 'Other'),
         key = str_remove_all(name, '(III|II|Jr\\.|Sr\\.|\\b[A-Z]\\.\\b|é)') %>%
           str_remove_all('[[:punct:]]') %>%
           str_replace_all('  ', ' ') %>%
           trimws() %>%
           tolower(),
         key = str_c(str_extract(key, '^[A-Za-z]+'), ' ',
                 str_extract(key, ' [A-Za-z]+$')))

candidates[is.na(candidates)] <- ''
ballotpedia[is.na(ballotpedia)] <- ''

## Fuzzy join to grab twitter handles from ballotpedia
fuzzy <- candidates %>%
  stringdist_join(
    select(ballotpedia, key, state, party_simple, district, twitter_name),
    by = 'key',
    mode = 'inner',
    method = 'lv',
    max_dist = 5,
    ignore_case = TRUE,
    distance_col = 'distance') %>%
  filter(state.x == state.y, 
         party_simple.x == party_simple.y,
         district.x == district.y)

fuzzy %>%
  filter(distance > 3) %>%
  arrange(key.x)

# kalena bruce	kyle labrue	MO	Republican
# carlos gimenez	carlos garin	FL	Republican	28
# thuy lowe	tuan le	FL	Republican
# john beatty	john henley	VA	Republican

candidates <- candidates %>%
  left_join(select(fuzzy, key.x, twitter_name),
            by = c('key' = 'key.x'),
            keep = FALSE) 

# %>%
#   unnest_longer(twitter_name, keep_empty = TRUE) %>%
#   filter(!is.na(twitter_name))
```

https://github.com/unitedstates/congress-legislators
https://www.propublica.org/datastore/dataset/politicians-tracked-by-politwoops

```{r, message = FALSE}
congress_current <- read_csv('https://theunitedstates.io/congress-legislators/legislators-current.csv')
congress_history <- read_csv('https://theunitedstates.io/congress-legislators/legislators-historical.csv')

congress <- rbind(congress_current, congress_history)

poliwoops = read_csv('https://s3.amazonaws.com/pp-projects-static/politwoops/active_accounts.csv')
```

Join

```{r}
candidates <- candidates %>%
  ## Join with IDs from congress data
  left_join(
    select(congress, bioguide_id, fec_ids, twitter_id),
    by = c('fec_id' = 'fec_ids'), suffix = c('','_congress'), keep = FALSE) %>%
  ## Join with IDs from poliwoops data based on bioguide ID
  left_join(
    select(poliwoops, bioguide_id, twitter_id) %>%
      filter(!is.na(bioguide_id)),
    by = 'bioguide_id', suffix = c('','_poliw_bio'), keep = FALSE) %>%
  ## Join with IDs from poliwoops data based on fec ID
  left_join(
    select(poliwoops, fec_candidate_id, twitter_id) %>%
      filter(!is.na(fec_candidate_id)), 
    by = c('fec_id' = 'fec_candidate_id'), suffix = c('','_poliw_fec'), keep = FALSE) # %>%

## Convert IDs to list, then unnest to long format
candidates <- candidates %>%
  unite(twitter_id, twitter_id, twitter_id_poliw_fec, twitter_id_poliw_bio, 
        sep = ',', na.rm = TRUE) %>%
  mutate(twitter_id = str_split(as.character(twitter_id), ",")) %>%
  unnest_longer(twitter_id, keep_empty = TRUE)

## Remove dupes, then unnest twitter_name to long
candidates <- candidates[!duplicated(candidates), ] %>%
  unnest_longer(twitter_name, keep_empty = TRUE)

candidates %>%
  filter(twitter_name != '' | twitter_id != '') %>%
  select(name) %>% unique() %>% nrow()

candidates
```


```{r}
write_csv(candidates, paste0('data/candidates',year,'.csv'))
```


