---
title: "DATA 607 - Project 3 - Data Cleaning / Tidying / Aggregation"
author: "Shoshana Farber"
date: "2023-03-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)

library(tidyverse)
library(snakecase)
library(DT)
```

## Introduction 

This work is the second of five stages of an analysis where the main objective was to identify the most valued data science skills. Our approach to this was to collect job postings from various job boards and extract the skills from the postings. The purpose of this specific file's work is to process the data from each of our data sources, stored in our github repository, and bring them all into one clean dataframes. In addition, this file also aggregates skills information into a dataframe to save a dictionary of skills for later use.

## Loading the Data

The first step was to load the data. Each of the data sources was obtained by web scraping from job boards which allow web scraping (USA jobs, AI jobs, dataanalyst.com) or downloading the job listings directly (NYC jobs). The scraping of each job board was done using python and a csv file was created for each and uploaded to our github repository so that it could be easily available in R for data analysis. 

```{r load-data}
# ai jobs web scrape csv
ai_jobs <- read.csv(url("https://github.com/kac624/D607_Project3/blob/main/data/aiJobsDf.csv?raw=true"), na.strings = c(""))

# usa jobs web scrape csvs
usa_jobs_data_analyst <- read.csv(url("https://raw.githubusercontent.com/kac624/D607_Project3/main/data/data_analyst_usa_jobs.csv"), na.strings = c(""))

usa_jobs_data_scientist <- read.csv(url("https://raw.githubusercontent.com/kac624/D607_Project3/main/data/data_scientist_usa_jobs.csv"), na.strings = c(""))

# combine usa jobs
usa_jobs <- rbind(usa_jobs_data_analyst, usa_jobs_data_scientist)

# nyc jobs csv download
nyc_jobs <- read.csv(url("https://github.com/kac624/D607_Project3/blob/main/data/NYC_Jobs-2.csv?raw=true"), na.strings = c(""))

# data analyst web scrape
data_analyst_jobs <- read.csv(url("https://github.com/kac624/D607_Project3/blob/main/data/data_analyst_dot_com.csv?raw=true"))
```

## Data Cleaning and Skills Dictionary

The next step in the process was to synthesize all of the data from each of the job postings and additionally create a skills dictionary to use later for extracting the skills from the job postings. Then, to simplify downstream processing, one common dataframe for all of the jobs posting data, `job_listings` and another dataframe for all of the skills, `skills` was created. The formats of the dataframes created are:

`job_listings`
- job_id
- job_title
- location
- salary_currency
- salary_min
- salary_max
- job_quals
- job_description
- website

`skills`
- skill

### AI Jobs

This data was web scraped by Keith Collela from [ai-jobs.net](https://ai-jobs.net/) and combined into a csv file. 

A look at the data frame for the AI Jobs listings showed that the `tags` column contains the data science skills that are required for the job. There are also some skills that may be listed within the `description` of the job listing. 

The job titles can be found in the `url` column within the url for the job listing. The format for each url is "https://ai-jobs.net//job/#####-job-title/". The number portion of the url was used as an ID for the jobs in this data frame. 

```{r}
# extracting job id and job titles
ai_jobs_cleaned <- ai_jobs %>%
  mutate("job_id" = str_extract(ai_jobs$url, '[0-9]+'),
         "job_title" = str_extract(ai_jobs$url, '[0-9]+-[a-z(-?)]+'), 
         job_title = str_remove_all(job_title, "(\\d|/)"),
         job_title = str_replace_all(job_title, "-", " "))
```

Next the job type, level, and salary ranges from the `ai_jobs` were extracted into a data frame:

```{r clean-ai-jobs}
ai_jobs_cleaned <- ai_jobs_cleaned %>%
  mutate("job_type" = str_extract(pay_level, "[A-Za-z]+(\\s[A-Za-z]+)?"),
         "job_level" = str_extract(pay_level, "[A-Za-z]+-level"),
         "salary_range" = str_extract(pay_level, "[A-Z]+\\s\\w+(\\+)?(\\s-\\s\\w+)?"),
         "salary_currency" = str_extract(salary_range, "[A-Z]+"),
         "salary_min" = as.numeric(str_extract(salary_range, "\\d+")) * 1000,
         "salary_max" = as.numeric(str_remove(str_extract(salary_range, "-\\s\\d+"), "-\\s")) * 1000)

ai_jobs_cleaned <- ai_jobs_cleaned %>%
  transmute(job_id,
            job_title,
            location, 
            salary_currency,
            salary_min,
            salary_max,
            "job_quals" = tags,
            "job_description" = description,
            "website" = "ai-jobs.net")
```

### Create a Skills Dictionary

The skills here were extracted from what was originally the `tags` column from AI jobs and that has been renamed as `job_quals`.

```{r get-ai-skills}
########## extract skills from tags ##############

# create a list of the skills from each job posting
jobs_skills <- str_split(ai_jobs_cleaned$job_quals, ";(\\s)?")

# loop through each listing to split up all the skills and put it in a separate data frame
for (job in 1:length(ai_jobs_cleaned$job_id)) {
  temp_skills <- unlist(jobs_skills[job])
  
  temp <- data.frame("job_id" = rep(ai_jobs_cleaned$job_id[job], length(temp_skills)),
                     "skill" = temp_skills)
  
  if (job == 1) {
    ai_jobs_skills <- temp
  } else {
    ai_jobs_skills <- rbind(ai_jobs_skills, temp)
  }
}
```

The first step was to initialize a dictionary with the skills extracted from the AI Jobs listings. Then this was combined with a data set of skills that was compiled based on research for useful data science skills - stored in our github repository.

```{r skills-dictionary}
# initialize dictionary
skills_dictionary <- unique(ai_jobs_skills$skill)

# additional skills 
additional_skills <- read.csv(url("https://raw.githubusercontent.com/kac624/D607_Project3/main/data/ds_skills.csv"))
additional_skills <- additional_skills$skill

# add to skills_dictionary and remove redundancy
skills_dictionary <- c(skills_dictionary, additional_skills)
skills_dictionary <- unique(skills_dictionary)
skills_dictionary <- skills_dictionary[!is.na(skills_dictionary)]

# making sure there is no duplicate (based on capitalization)
length(unique(tolower(skills_dictionary))) == length(skills_dictionary)

# checking for redundant values
redundancy_check <- data.frame("skill" = skills_dictionary,
                               "lower_skill" = tolower(skills_dictionary))
                               
redundancy_check %>%
  count(lower_skill) %>%
  filter(n > 1)

redundancy_check %>%
  filter(lower_skill == "matlab")

# remove repeated value
skills_dictionary <- skills_dictionary[!skills_dictionary == "Matlab"]

length(unique(tolower(skills_dictionary))) == length(skills_dictionary)

df_skills <- as.data.frame(skills_dictionary)
```

### USA Jobs

This csv file was generated by Kayleah Griffen by web scraping [usajobs.gov](https://www.usajobs.gov/). 

The data frame already includes a column for job title. The number from the url posting will become the `job_id`. The salary and skills needs to be extracted. Also - in USA jobs, although Data Scientist and Data Analyst were explicity searched - some results irrelevant to Data Science/ Analysis came up - these were filtered out - only titles with data in the name as well as scientist, or engineer, or analyst, or developer were included. 

```{r clean-usa-jobs}
usa_jobs_cleaned <- usa_jobs %>%
  mutate("job_id" = str_extract(job_url, "\\d+"),
         job_title = str_remove(job_title, "\\s+"),
         job_summary = str_remove(job_summary, "\\s+Summary\\s+"),
         job_salary = str_extract(job_salary, "\\d+([-\\d]+)\\d"),
         "salary_currency" = "USD",
         "salary_min" = as.numeric(str_remove_all(str_extract(job_salary, "\\d+-\\d+-"), "-")),
         "salary_max" = as.numeric(str_remove_all(str_extract(job_salary, "-\\d+-\\d+"), "-")),
         job_quals = str_remove(job_quals, "\\s+Qualifications\\s+"),
         job_duties = str_remove(job_duties, "\\s+Help\\s+Duties\\s+"),
         location = case_when(str_detect(job_location, '(Anywhere|remote)') ~ 'Remote',
                              str_detect(job_location, 'Washington') ~ 'Washington DC',
                              TRUE ~ str_extract(job_location, '(?<= )[A-Z]{2}(?= )')))

usa_jobs_cleaned <- usa_jobs_cleaned %>%
  transmute(job_id,
            job_title,
            location,
            salary_currency,
            salary_min,
            salary_max,
            job_quals,
            "job_description" = paste(job_summary,job_duties),
            "website" = "usajobs.gov") %>%
  filter(!duplicated(job_id))

usa_jobs_cleaned <- usa_jobs_cleaned %>% 
  filter(str_detect(tolower(job_title), "data")) %>%
  filter(str_detect(tolower(job_title), "(analyst|engineer|developer|scientist)"))
```

### NYC Jobs

Next, the NYC jobs data was brought in. Similar to USA jobs, NYC jobs also had non data related jobs listed so again those were filtered out. 

```{r clean-nyc-jobs}
# change column titles to snake_case
names(nyc_jobs) <- to_snake_case(names(nyc_jobs))

nyc_jobs_filtered <- nyc_jobs %>% 
  filter(str_detect(tolower(business_title), "data")) %>%
  filter(str_detect(tolower(business_title), "(analyst|engineer|developer|scientist)"))

nyc_jobs_cleaned <- nyc_jobs_filtered %>%
  transmute(job_id,
            "job_title" = business_title,
            "location" = "New York",
            salary_currency = "USD",
            "salary_min" = salary_range_from,
            "salary_max" = salary_range_to,
            "job_quals" = preferred_skills,
            job_description,
            "website" = "data.cityofnewyork.us") %>%
  filter(!is.na(job_quals))
```

### Data Analyst Jobs

This csv file was generated by Kayleah Griffen by web scraping [dataanalyst.com](https://www.dataanalyst.com/). 

The data frame already included a column for job title. There is no indication of job id for any of the listing, so the `X` column will be used as the `job_id`. Skills will be extracted from the `job_requirement` and `job_roles` columns renamed as `job_quals` and `job_descriptions` to fit the format previously created for the other data frames. 

The job id here seems to be a combination of the job title and then a series of number or letters afterward. 

```{r clean-data-analyst-jobs}
# determine countries in the data set for salary information
data_analyst_jobs %>%
  count(job_country)

# str_view(data_analyst_jobs$job_url, "\\w+-.+")

# clean the data frame
data_analyst_jobs_cleaned <- data_analyst_jobs %>%
    mutate("job_id" = str_extract(job_url, "\\w+-.+"),
           "salary_currency" = ifelse(job_country == "United States", "USD", "GBP"),
           "salary_min" =  as.numeric(str_remove_all(str_extract(job_salary, "-\\d+-"), "-")),
           "salary_max" = as.numeric(str_remove_all(str_extract(job_salary, "-\\d+$"), "-")))
  
data_analyst_jobs_cleaned <- data_analyst_jobs_cleaned %>%
  transmute(job_id,
            job_title,
            "location" = job_country,
            salary_currency,
            salary_min,
            salary_max,
            "job_quals" = str_remove(job_requirements, "\\s+"),
            "job_description" = str_remove(job_roles, "\\s+"),
            "website" = "dataanalyst.com") %>%
  filter(!is.na(job_quals))

# making sure each job_id is unique
length(unique(data_analyst_jobs_cleaned$job_id)) == length(data_analyst_jobs_cleaned$job_id)
```

## Combining Data Frames

To create a full data frame of all the job listing the jobs data sets were merged into `job_listings` and a dataframe with the skills was also created, `skills`.

```{r combine-df}
#join listings data frames and eliminate redundancies
job_listings <- ai_jobs_cleaned %>%
  rbind(usa_jobs_cleaned) %>%
  rbind(nyc_jobs_cleaned) %>%
  rbind(data_analyst_jobs_cleaned) %>%
  unique()

skills <- data.frame(skill = skills_dictionary)
```

In order to account for looser terminology in certain cases, (ex. "Data visualization" as "visual"), the skills dictionary was amended to have the listed skill, as well as the search term. The search term in all cases was converted to lowercase for the comparison. 

```{r}
skills <- skills %>%
  mutate("search_word" = tolower(skill),
         search_word = replace(search_word, search_word == "data visualization", "visual"),
         search_word = replace(search_word, search_word == "collaboration", "collabor"),
         search_word = replace(search_word, search_word == "communcation", "communicat"))

skills[nrow(skills)+1,] <- c("Data analysis", "analyze data")

skills$search_word <- skills$search_word %>%
  str_replace_all('[â¢\u0080-\uFFFF]', '') %>%
  str_replace_all('[‘’“”\\,\\.\\(\\)\\;\\:\'\\?]','') %>%
  str_replace_all('[-]', ' ')
```

Separate CSV files for each data frame were created to be analyzed later.

```{r write-csvs, eval=F}
write.csv(job_listings, "output/job_listings.csv", row.names=F)

write.csv(skills, "output/skills.csv", row.names=F)
```

## Conclusion

For this stage of the analysis, the objective to create clean dataframes - one for skills and one for job posting information - was met. Some notes on limitations and possible extensions of this work are:

1) The job postings data in the github repository will become dated. If someone was interested in updating the data they would need to run the python files for scraping again and re-upload the files to the github repository.

2) Our skills dictionary relies on a static list of skill and the AI jobs skills tags. The list could become dated if new data science skills emerge that are not represented in AI jobs or the static skills list. Potentially an unstructured learning approach - where skills are detected from all jobs postings - could be implemented to combat this. 

3) Duplicated job listing across job boards was not checked for. To extend this work we would find a robust method for checking for duplication such as including the company in the `job_listings` dataframe and checking for exact matches of the rest of the columns of the dataframe aside from `job_id` and `website`.

Overall, our processing of the job listings and creating a skills dictionary was successful and provides a foundation for the next stage of the project which is to extract the skills from the job postings. 