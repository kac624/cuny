{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# functional\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# custom\n",
    "from scripts.classes import RnnTextClassifier, RnnDataset\n",
    "from scripts.functions import train_rnn, rnn_bce_logits_predict\n",
    "\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = [\n",
    "    'tfidf_train', 'w2v_train', 'bert_train', 'gpt_train', \n",
    "    'tfidf_test', 'w2v_test', 'bert_test', 'gpt_test', \n",
    "    'y_train', 'y_test'\n",
    "]\n",
    "\n",
    "for var_name in data_names:\n",
    "    with open(f'data/{var_name}.pkl', 'rb') as f:\n",
    "        globals()[var_name] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_int = {label: idx for idx, label in enumerate(np.unique(y_train))}\n",
    "int_to_label = {idx: label for idx, label in enumerate(np.unique(y_train))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binzarize Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'barely-true',\n",
       " 1: 'false',\n",
       " 2: 'half-true',\n",
       " 3: 'mostly-true',\n",
       " 4: 'pants-fire',\n",
       " 5: 'true'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Train---\n",
      "tensor([0.5617, 0.4383])\n",
      "---Test---\n",
      "tensor([0.5635, 0.4365])\n"
     ]
    }
   ],
   "source": [
    "false = ['false', 'pants-fire', 'barely-true']\n",
    "true = ['half-true', 'mostly-true', 'true']\n",
    "\n",
    "y_train_binary = torch.tensor(np.where(y_train.isin(false), 1, 0), dtype = torch.float)\n",
    "y_test_binary = torch.tensor(np.where(y_test.isin(false), 1, 0), dtype = torch.float)\n",
    "\n",
    "print(\n",
    "    f'---Train---\\n{y_train_binary.unique(return_counts = True)[1] / len(y_train_binary)}\\n'\n",
    "    f'---Test---\\n{y_test_binary.unique(return_counts = True)[1] / len(y_test_binary)}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/y_train_binary.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train_binary, f)\n",
    "\n",
    "with open(f'data/y_test_binary.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test_binary, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain models on binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.perf_counter()\n",
    "\n",
    "# for X_train, X_name in zip([tfidf_train, w2v_train, bert_train, gpt_train], ['tfidf', 'w2v', 'bert', 'gpt']):\n",
    "\n",
    "#     # recurrent neural network\n",
    "#     rnn = RnnTextClassifier(\n",
    "#         input_size = X_train.shape[1], output_size = 1, \n",
    "#         hidden_size = 256, num_layers = 2, dropout = 0.5\n",
    "#     )\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "#     optimizer = torch.optim.Adam(rnn.parameters(), lr = 0.001)\n",
    "#     n_epochs = range(100)\n",
    "#     dataset = RnnDataset(X_train, y_train_binary.unsqueeze(1))\n",
    "#     data_loader = DataLoader(dataset, batch_size = int(X_train.shape[0] / 128), shuffle = True)\n",
    "#     trained_rnn = train_rnn(rnn, data_loader, criterion, optimizer, n_epochs)\n",
    "#     torch.save(trained_rnn, f'models/rnn_binary_{X_name}.pth')\n",
    "#     print(f'Finished with RNN-{X_name} - Time elapsed: {(time.perf_counter()-start_time)/60:.2f}\\n')\n",
    "\n",
    "#     # logistic regression\n",
    "#     lr = LogisticRegression(max_iter = 4000, penalty='l2', C = 0.1)\n",
    "#     lr.fit(X_train, y_train_binary)\n",
    "#     joblib.dump(lr, f'models/lr_binary_{X_name}.joblib')\n",
    "#     print(f'Finished with LR-{X_name} - Time elapsed: {(time.perf_counter()-start_time)/60:.2f}\\n')\n",
    "\n",
    "#     # random forest\n",
    "#     rf = RandomForestClassifier(n_estimators = 1000, max_depth = 100, max_features = 'log2')\n",
    "#     rf.fit(X_train, y_train_binary)\n",
    "#     joblib.dump(rf, f'models/rf_binary_{X_name}.joblib')\n",
    "#     print(f'Finished with RF-{X_name} - Time elapsed: {(time.perf_counter()-start_time)/60:.2f}\\n')\n",
    "        \n",
    "#     # support vector machine\n",
    "#     svm = SVC(kernel = 'linear')\n",
    "#     svm.fit(X_train, y_train_binary)\n",
    "#     joblib.dump(svm, f'models/svm_binary_{X_name}.joblib')\n",
    "#     print(f'Finished with SVM-{X_name} - Time elapsed: {(time.perf_counter()-start_time)/60:.2f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tfidf = joblib.load('models/lr_binary_tfidf.joblib')\n",
    "lr_w2v = joblib.load('models/lr_binary_w2v.joblib')\n",
    "lr_bert = joblib.load('models/lr_binary_bert.joblib')\n",
    "lr_gpt = joblib.load('models/lr_binary_gpt.joblib')\n",
    "\n",
    "rf_tfidf = joblib.load('models/rf_binary_tfidf.joblib')\n",
    "rf_w2v = joblib.load('models/rf_binary_w2v.joblib')\n",
    "rf_bert = joblib.load('models/rf_binary_bert.joblib')\n",
    "rf_gpt = joblib.load('models/rf_binary_gpt.joblib')\n",
    "\n",
    "svm_tfidf = joblib.load('models/svm_binary_tfidf.joblib')\n",
    "svm_w2v = joblib.load('models/svm_binary_w2v.joblib')\n",
    "svm_bert = joblib.load('models/svm_binary_bert.joblib')\n",
    "svm_gpt = joblib.load('models/svm_binary_gpt.joblib')\n",
    "\n",
    "rnn_tfidf = torch.load('models/rnn_binary_tfidf.pth')\n",
    "rnn_w2v = torch.load('models/rnn_binary_w2v.pth')\n",
    "rnn_bert = torch.load('models/rnn_binary_bert.pth')\n",
    "rnn_gpt = torch.load('models/rnn_binary_gpt.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    rnn_tfidf, rnn_w2v, rnn_bert, rnn_gpt,\n",
    "    lr_tfidf, lr_w2v, lr_bert, lr_gpt,\n",
    "    rf_tfidf, rf_w2v, rf_bert, rf_gpt, \n",
    "    svm_tfidf, svm_w2v, svm_bert, svm_gpt\n",
    "]\n",
    "\n",
    "data_sets = [\n",
    "    (tfidf_train, tfidf_test), (w2v_train, w2v_test), \n",
    "    (bert_train, bert_test), (gpt_train, gpt_test),\n",
    "]\n",
    "\n",
    "data_names = ['tfidf', 'w2v', 'bert', 'gpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RnnTextClassifier(\n",
      "  (rnn): RNN(1000, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "Data: tfidf\n",
      "In-sample accuracy: 0.825\n",
      "Out-of-sample accuracy: 0.558\n",
      "Out-of-sample F1: 0.511\n",
      "Confusion Matrix (OOS):\n",
      "[[0.58 0.42]\n",
      " [0.47 0.53]]\n",
      "\n",
      "----------\n",
      "\n",
      "Model: RnnTextClassifier(\n",
      "  (rnn): RNN(1000, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "Data: w2v\n",
      "In-sample accuracy: 0.591\n",
      "Out-of-sample accuracy: 0.579\n",
      "Out-of-sample F1: 0.476\n",
      "Confusion Matrix (OOS):\n",
      "[[0.689 0.311]\n",
      " [0.562 0.438]]\n",
      "\n",
      "----------\n",
      "\n",
      "Model: RnnTextClassifier(\n",
      "  (rnn): RNN(768, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "Data: bert\n",
      "In-sample accuracy: 0.922\n",
      "Out-of-sample accuracy: 0.612\n",
      "Out-of-sample F1: 0.475\n",
      "Confusion Matrix (OOS):\n",
      "[[0.776 0.224]\n",
      " [0.599 0.401]]\n",
      "\n",
      "----------\n",
      "\n",
      "Model: RnnTextClassifier(\n",
      "  (rnn): RNN(768, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "Data: gpt\n",
      "In-sample accuracy: 0.698\n",
      "Out-of-sample accuracy: 0.587\n",
      "Out-of-sample F1: 0.360\n",
      "Confusion Matrix (OOS):\n",
      "[[0.836 0.164]\n",
      " [0.734 0.266]]\n",
      "\n",
      "----------\n",
      "\n",
      "Model: LogisticRegression(C=0.1, max_iter=4000)\n",
      "Data: tfidf\n",
      "In-sample accuracy: 0.636\n",
      "Out-of-sample accuracy: 0.607\n",
      "Out-of-sample F1: 0.362\n",
      "Confusion Matrix (OOS):\n",
      "[[0.88  0.12 ]\n",
      " [0.745 0.255]]\n",
      "\n",
      "----------\n",
      "\n",
      "Model: LogisticRegression(C=0.1, max_iter=4000)\n",
      "Data: w2v\n",
      "In-sample accuracy: 0.589\n",
      "Out-of-sample accuracy: 0.597\n",
      "Out-of-sample F1: 0.430\n",
      "Confusion Matrix (OOS):\n",
      "[[0.789 0.211]\n",
      " [0.651 0.349]]\n",
      "\n",
      "----------\n",
      "\n",
      "Model: LogisticRegression(C=0.1, max_iter=4000)\n",
      "Data: bert\n",
      "In-sample accuracy: 0.664\n",
      "Out-of-sample accuracy: 0.621\n",
      "Out-of-sample F1: 0.462\n",
      "Confusion Matrix (OOS):\n",
      "[[0.814 0.186]\n",
      " [0.627 0.373]]\n",
      "\n",
      "----------\n",
      "\n",
      "Model: LogisticRegression(C=0.1, max_iter=4000)\n",
      "Data: gpt\n",
      "In-sample accuracy: 0.672\n",
      "Out-of-sample accuracy: 0.586\n",
      "Out-of-sample F1: 0.413\n",
      "Confusion Matrix (OOS):\n",
      "[[0.78  0.22 ]\n",
      " [0.665 0.335]]\n",
      "\n",
      "----------\n",
      "\n",
      "Model: RandomForestClassifier(max_depth=100, max_features='log2', n_estimators=1000)\n",
      "Data: tfidf\n",
      "In-sample accuracy: 0.960\n",
      "Out-of-sample accuracy: 0.600\n",
      "Out-of-sample F1: 0.393\n",
      "Confusion Matrix (OOS):\n",
      "[[0.835 0.165]\n",
      " [0.703 0.297]]\n",
      "\n",
      "----------\n",
      "\n",
      "Model: RandomForestClassifier(max_depth=100, max_features='log2', n_estimators=1000)\n",
      "Data: w2v\n",
      "In-sample accuracy: 1.000\n",
      "Out-of-sample accuracy: 0.583\n",
      "Out-of-sample F1: 0.481\n",
      "Confusion Matrix (OOS):\n",
      "[[0.692 0.308]\n",
      " [0.557 0.443]]\n",
      "\n",
      "----------\n",
      "\n",
      "Model: RandomForestClassifier(max_depth=100, max_features='log2', n_estimators=1000)\n",
      "Data: bert\n",
      "In-sample accuracy: 1.000\n",
      "Out-of-sample accuracy: 0.600\n",
      "Out-of-sample F1: 0.344\n",
      "Confusion Matrix (OOS):\n",
      "[[0.878 0.122]\n",
      " [0.759 0.241]]\n",
      "\n",
      "----------\n",
      "\n",
      "Model: RandomForestClassifier(max_depth=100, max_features='log2', n_estimators=1000)\n",
      "Data: gpt\n",
      "In-sample accuracy: 1.000\n",
      "Out-of-sample accuracy: 0.588\n",
      "Out-of-sample F1: 0.283\n",
      "Confusion Matrix (OOS):\n",
      "[[0.899 0.101]\n",
      " [0.814 0.186]]\n",
      "\n",
      "----------\n",
      "\n",
      "Model: SVC(kernel='linear')\n",
      "Data: tfidf\n",
      "In-sample accuracy: 0.671\n",
      "Out-of-sample accuracy: 0.605\n",
      "Out-of-sample F1: 0.489\n",
      "Confusion Matrix (OOS):\n",
      "[[0.737 0.263]\n",
      " [0.566 0.434]]\n",
      "\n",
      "----------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     y_pred_test \u001b[38;5;241m=\u001b[39m rnn_bce_logits_predict(model, X_test)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     y_pred_train \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train)\n\u001b[0;32m     12\u001b[0m     y_pred_test \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     13\u001b[0m accuracy_train \u001b[38;5;241m=\u001b[39m accuracy_score(y_train_binary, y_pred_train)\n",
      "File \u001b[1;32mc:\\Users\\Keith\\miniconda3\\envs\\cuny\\Lib\\site-packages\\sklearn\\svm\\_base.py:820\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    818\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp))\n",
      "File \u001b[1;32mc:\\Users\\Keith\\miniconda3\\envs\\cuny\\Lib\\site-packages\\sklearn\\svm\\_base.py:435\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    433\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_for_predict(X)\n\u001b[0;32m    434\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[1;32m--> 435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[1;32mc:\\Users\\Keith\\miniconda3\\envs\\cuny\\Lib\\site-packages\\sklearn\\svm\\_base.py:454\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    447\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape[1] = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be equal to \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe number of samples at training time\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    449\u001b[0m             \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    450\u001b[0m         )\n\u001b[0;32m    452\u001b[0m svm_type \u001b[38;5;241m=\u001b[39m LIBSVM_IMPL\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl)\n\u001b[1;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m libsvm\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m    455\u001b[0m     X,\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dual_coef_,\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intercept_,\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[0;32m    463\u001b[0m     svm_type\u001b[38;5;241m=\u001b[39msvm_type,\n\u001b[0;32m    464\u001b[0m     kernel\u001b[38;5;241m=\u001b[39mkernel,\n\u001b[0;32m    465\u001b[0m     degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegree,\n\u001b[0;32m    466\u001b[0m     coef0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef0,\n\u001b[0;32m    467\u001b[0m     gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gamma,\n\u001b[0;32m    468\u001b[0m     cache_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_size,\n\u001b[0;32m    469\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "columns = ['model', 'data', 'accuracy_is', 'accuracy_oos', 'precision_oos', 'recall_oos', 'f1_oos']\n",
    "\n",
    "results = pd.DataFrame(columns = columns)\n",
    "\n",
    "for model, data, data_name in zip(models, data_sets*4, data_names*4):\n",
    "    X_train, X_test = data\n",
    "    if 'Rnn' in str(model):\n",
    "        y_pred_train = rnn_bce_logits_predict(model, X_train)\n",
    "        y_pred_test = rnn_bce_logits_predict(model, X_test)\n",
    "    else:\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "    accuracy_train = accuracy_score(y_train_binary, y_pred_train)\n",
    "    accuracy_test = accuracy_score(y_test_binary, y_pred_test)\n",
    "    precision_oos = precision_score(y_test_binary, y_pred_test)\n",
    "    recall_oos = recall_score(y_test_binary, y_pred_test)\n",
    "    f1_oos = f1_score(y_test_binary, y_pred_test)\n",
    "    confusion_oos = confusion_matrix(y_test_binary, y_pred_test, normalize=\"true\")\n",
    "\n",
    "    result = dict(zip(columns, [\n",
    "        str(model), data_name, accuracy_train, accuracy_test, \n",
    "        precision_oos, recall_oos, f1_oos\n",
    "    ]))\n",
    "    results = pd.concat([results, pd.DataFrame(result, index=[0])], ignore_index=True)\n",
    "\n",
    "    print(\n",
    "        f'Model: {model}\\n'\n",
    "        f'Data: {data_name}\\n'\n",
    "        f'In-sample accuracy: {accuracy_train:.3f}\\n'\n",
    "        f'Out-of-sample accuracy: {accuracy_test:.3f}\\n'\n",
    "        f'Out-of-sample F1: {f1_oos:.3f}\\n'\n",
    "        f'Confusion Matrix (OOS):\\n{np.round(confusion_oos, 3)}\\n'\n",
    "        f'\\n----------\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>data</th>\n",
       "      <th>accuracy_is</th>\n",
       "      <th>accuracy_oos</th>\n",
       "      <th>precision_oos</th>\n",
       "      <th>recall_oos</th>\n",
       "      <th>f1_oos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RnnTextClassifier(\\n  (rnn): RNN(1000, 256, nu...</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.825391</td>\n",
       "      <td>0.558011</td>\n",
       "      <td>0.494098</td>\n",
       "      <td>0.529837</td>\n",
       "      <td>0.511344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RnnTextClassifier(\\n  (rnn): RNN(1000, 256, nu...</td>\n",
       "      <td>w2v</td>\n",
       "      <td>0.591016</td>\n",
       "      <td>0.579321</td>\n",
       "      <td>0.521552</td>\n",
       "      <td>0.437613</td>\n",
       "      <td>0.475910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RnnTextClassifier(\\n  (rnn): RNN(768, 256, num...</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.922168</td>\n",
       "      <td>0.612470</td>\n",
       "      <td>0.581152</td>\n",
       "      <td>0.401447</td>\n",
       "      <td>0.474866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RnnTextClassifier(\\n  (rnn): RNN(768, 256, num...</td>\n",
       "      <td>gpt</td>\n",
       "      <td>0.698047</td>\n",
       "      <td>0.587214</td>\n",
       "      <td>0.556818</td>\n",
       "      <td>0.265823</td>\n",
       "      <td>0.359853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=4000)</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.635645</td>\n",
       "      <td>0.606946</td>\n",
       "      <td>0.621145</td>\n",
       "      <td>0.254973</td>\n",
       "      <td>0.361538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=4000)</td>\n",
       "      <td>w2v</td>\n",
       "      <td>0.588574</td>\n",
       "      <td>0.596685</td>\n",
       "      <td>0.561047</td>\n",
       "      <td>0.349005</td>\n",
       "      <td>0.430323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=4000)</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.663965</td>\n",
       "      <td>0.621152</td>\n",
       "      <td>0.607670</td>\n",
       "      <td>0.372514</td>\n",
       "      <td>0.461883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=4000)</td>\n",
       "      <td>gpt</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>0.585635</td>\n",
       "      <td>0.540936</td>\n",
       "      <td>0.334539</td>\n",
       "      <td>0.413408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RandomForestClassifier(max_depth=100, max_feat...</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.959863</td>\n",
       "      <td>0.599842</td>\n",
       "      <td>0.581560</td>\n",
       "      <td>0.296564</td>\n",
       "      <td>0.392814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RandomForestClassifier(max_depth=100, max_feat...</td>\n",
       "      <td>w2v</td>\n",
       "      <td>0.999609</td>\n",
       "      <td>0.583268</td>\n",
       "      <td>0.526882</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.481336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RandomForestClassifier(max_depth=100, max_feat...</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.999609</td>\n",
       "      <td>0.599842</td>\n",
       "      <td>0.604545</td>\n",
       "      <td>0.240506</td>\n",
       "      <td>0.344114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RandomForestClassifier(max_depth=100, max_feat...</td>\n",
       "      <td>gpt</td>\n",
       "      <td>0.999609</td>\n",
       "      <td>0.588003</td>\n",
       "      <td>0.588571</td>\n",
       "      <td>0.186257</td>\n",
       "      <td>0.282967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SVC(kernel='linear')</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.671289</td>\n",
       "      <td>0.604578</td>\n",
       "      <td>0.560748</td>\n",
       "      <td>0.433996</td>\n",
       "      <td>0.489297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SVC(kernel='linear')</td>\n",
       "      <td>w2v</td>\n",
       "      <td>0.570508</td>\n",
       "      <td>0.576164</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.079566</td>\n",
       "      <td>0.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SVC(kernel='linear')</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.677344</td>\n",
       "      <td>0.614049</td>\n",
       "      <td>0.586486</td>\n",
       "      <td>0.392405</td>\n",
       "      <td>0.470206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SVC(kernel='linear')</td>\n",
       "      <td>gpt</td>\n",
       "      <td>0.685352</td>\n",
       "      <td>0.561957</td>\n",
       "      <td>0.497462</td>\n",
       "      <td>0.354430</td>\n",
       "      <td>0.413939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model   data  accuracy_is  \\\n",
       "0   RnnTextClassifier(\\n  (rnn): RNN(1000, 256, nu...  tfidf     0.825391   \n",
       "1   RnnTextClassifier(\\n  (rnn): RNN(1000, 256, nu...    w2v     0.591016   \n",
       "2   RnnTextClassifier(\\n  (rnn): RNN(768, 256, num...   bert     0.922168   \n",
       "3   RnnTextClassifier(\\n  (rnn): RNN(768, 256, num...    gpt     0.698047   \n",
       "4            LogisticRegression(C=0.1, max_iter=4000)  tfidf     0.635645   \n",
       "5            LogisticRegression(C=0.1, max_iter=4000)    w2v     0.588574   \n",
       "6            LogisticRegression(C=0.1, max_iter=4000)   bert     0.663965   \n",
       "7            LogisticRegression(C=0.1, max_iter=4000)    gpt     0.671875   \n",
       "8   RandomForestClassifier(max_depth=100, max_feat...  tfidf     0.959863   \n",
       "9   RandomForestClassifier(max_depth=100, max_feat...    w2v     0.999609   \n",
       "10  RandomForestClassifier(max_depth=100, max_feat...   bert     0.999609   \n",
       "11  RandomForestClassifier(max_depth=100, max_feat...    gpt     0.999609   \n",
       "12                               SVC(kernel='linear')  tfidf     0.671289   \n",
       "13                               SVC(kernel='linear')    w2v     0.570508   \n",
       "14                               SVC(kernel='linear')   bert     0.677344   \n",
       "15                               SVC(kernel='linear')    gpt     0.685352   \n",
       "\n",
       "    accuracy_oos  precision_oos  recall_oos    f1_oos  \n",
       "0       0.558011       0.494098    0.529837  0.511344  \n",
       "1       0.579321       0.521552    0.437613  0.475910  \n",
       "2       0.612470       0.581152    0.401447  0.474866  \n",
       "3       0.587214       0.556818    0.265823  0.359853  \n",
       "4       0.606946       0.621145    0.254973  0.361538  \n",
       "5       0.596685       0.561047    0.349005  0.430323  \n",
       "6       0.621152       0.607670    0.372514  0.461883  \n",
       "7       0.585635       0.540936    0.334539  0.413408  \n",
       "8       0.599842       0.581560    0.296564  0.392814  \n",
       "9       0.583268       0.526882    0.443038  0.481336  \n",
       "10      0.599842       0.604545    0.240506  0.344114  \n",
       "11      0.588003       0.588571    0.186257  0.282967  \n",
       "12      0.604578       0.560748    0.433996  0.489297  \n",
       "13      0.576164       0.611111    0.079566  0.140800  \n",
       "14      0.614049       0.586486    0.392405  0.470206  \n",
       "15      0.561957       0.497462    0.354430  0.413939  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuny",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
