{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# nlp processing / cleaning\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "# vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# functional\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/liars_train.csv')\n",
    "test = pd.read_csv('data/liars_test.csv')\n",
    "valid = pd.read_csv('data/liars_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Says the Annies List political group supports third-trimester abortions on demand.\n"
     ]
    }
   ],
   "source": [
    "print(train.statement[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up text - Lemmitize, NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "train.statement = train.statement.apply(lambda x: ' '.join([token.lemma_.lower() for token in nlp(x)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering - TF-IDF, Word2Vec, BERT Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10240, 1000]) torch.Size([1267, 1000])\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words = 'english', ngram_range = (1,2), max_features = 1000)\n",
    "tfidf_train = tfidf.fit_transform(train.statement)\n",
    "tfidf_test = tfidf.transform(test.statement)\n",
    "\n",
    "tfidf_train = torch.tensor(tfidf_train.toarray(), dtype = torch.float)\n",
    "tfidf_test = torch.tensor(tfidf_test.toarray(), dtype = torch.float)\n",
    "\n",
    "print(tfidf_train.shape, tfidf_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Keith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "nltk.download('punkt')\n",
    "\n",
    "tokenized_statements_train = [nltk.tokenize.word_tokenize(statement.lower()) for statement in train.statement]\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences = tokenized_statements_train, \n",
    "    vector_size = 1000, window = 5, min_count = 1, workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(statement, model):\n",
    "    words = [word for word in statement if word in model.wv.key_to_index]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean(model.wv[words], axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "w2v_train = pd.Series(tokenized_statements_train).apply(lambda x: get_sentence_vector(x, w2v_model))\n",
    "w2v_train = np.array(w2v_train.tolist())\n",
    "\n",
    "tokenized_statements_test = [nltk.tokenize.word_tokenize(statement.lower()) for statement in test.statement]\n",
    "w2v_test = pd.Series(tokenized_statements_test).apply(lambda x: get_sentence_vector(x, w2v_model))\n",
    "w2v_test = np.array(w2v_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10240, 1000) (1267, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(w2v_train.shape, w2v_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_bert(statement, tokenizer, model):\n",
    "    input = tokenizer(\n",
    "        statement, return_tensors = 'pt', \n",
    "        padding = True, truncation = True, max_length = 512\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(**input)\n",
    "    embeddings_vector = output.last_hidden_state.mean(dim = 1).squeeze()\n",
    "    \n",
    "    return embeddings_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10240, 768) (1267, 768)\n"
     ]
    }
   ],
   "source": [
    "bert_train = train.statement.apply(lambda x: get_embeddings_bert(x, tokenizer, model))\n",
    "bert_test = test.statement.apply(lambda x: get_embeddings_bert(x, tokenizer, model))\n",
    "\n",
    "bert_train = np.array(bert_train.tolist())\n",
    "bert_test = np.array(bert_test.tolist())\n",
    "\n",
    "print(bert_train.shape, bert_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_gpt(statement, tokenizer, model):\n",
    "    input = tokenizer(\n",
    "        statement, return_tensors = 'pt', # padding = True, \n",
    "        truncation = True, max_length = 512\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(**input)\n",
    "    embeddings_vector = output.last_hidden_state.mean(dim = 1).squeeze()\n",
    "    \n",
    "    return embeddings_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10240, 768) (1267, 768)\n"
     ]
    }
   ],
   "source": [
    "gpt_train = train.statement.apply(lambda x: get_embeddings_gpt(x, tokenizer, model))\n",
    "gpt_test = test.statement.apply(lambda x: get_embeddings_gpt(x, tokenizer, model))\n",
    "\n",
    "gpt_train = np.array(gpt_train.tolist())\n",
    "gpt_test = np.array(gpt_test.tolist())\n",
    "\n",
    "print(gpt_train.shape, gpt_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.label\n",
    "y_test = test.label\n",
    "\n",
    "# Prep labels for nn training\n",
    "label_to_int = {label: idx for idx, label in enumerate(np.unique(y_train))}\n",
    "y_train_tensor = np.array([label_to_int[label] for label in y_train])\n",
    "y_train_tensor = torch.tensor(y_train_tensor, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [tfidf_train, w2v_train, bert_train, gpt_train, tfidf_test, w2v_test, bert_test, gpt_test, y_train, y_test]\n",
    "data_names = ['tfidf_train', 'w2v_train', 'bert_train', 'gpt_train', 'tfidf_test', 'w2v_test', 'bert_test', 'gpt_test', 'y_train', 'y_test']\n",
    "\n",
    "for var_df, var_name in zip(data_list, data_names):\n",
    "    with open(f'data/{var_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(var_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnTextClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(RnnTextClassifier, self).__init__()\n",
    "\n",
    "        # model params\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # reshape input\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # initialize hidden state\n",
    "        hidden = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "\n",
    "        # get RNN output\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "class RnnDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "\n",
    "def train_rnn(model, data_loader, criterion, optimizer, n_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for _ in n_epochs:\n",
    "\n",
    "        for X_batch, y_batch in data_loader:\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tfidf_train.detach().clone()\n",
    "n_classes = len(y_train.unique())\n",
    "\n",
    "rnn = RnnTextClassifier(\n",
    "    input_size = X_train.shape[1], \n",
    "    output_size = len(y_train.unique()), \n",
    "    hidden_size = 256, \n",
    "    num_layers = 2\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr = 0.001)\n",
    "n_epochs = range(1)\n",
    "dataset = RnnDataset(X_train, y_train_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size = int(X_train.shape[0] / 128), shuffle = True)\n",
    "\n",
    "test = train_rnn(rnn, data_loader, criterion, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RnnDataset(X_train, y_train_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size = int(X_train.shape[0] / 128), shuffle = True)\n",
    "\n",
    "for X_batch, y_batch in data_loader:\n",
    "    y_pred = test(X_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished with RNN-tfidf - Time elapsed: 0.61\n",
      "\n",
      "Finished with LR-tfidf - Time elapsed: 0.63\n",
      "\n",
      "Finished with RF-tfidf - Time elapsed: 0.79\n",
      "\n",
      "Finished with SVM-tfidf - Time elapsed: 1.75\n",
      "\n",
      "Finished with RNN-w2v - Time elapsed: 2.38\n",
      "\n",
      "Finished with LR-w2v - Time elapsed: 2.43\n",
      "\n",
      "Finished with RF-w2v - Time elapsed: 2.95\n",
      "\n",
      "Finished with SVM-w2v - Time elapsed: 3.87\n",
      "\n",
      "Finished with RNN-bert - Time elapsed: 4.47\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Keith\\miniconda3\\envs\\cuny\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished with LR-bert - Time elapsed: 4.56\n",
      "\n",
      "Finished with RF-bert - Time elapsed: 4.95\n",
      "\n",
      "Finished with SVM-bert - Time elapsed: 5.58\n",
      "\n",
      "Finished with RNN-gpt - Time elapsed: 6.20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Keith\\miniconda3\\envs\\cuny\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished with LR-gpt - Time elapsed: 6.29\n",
      "\n",
      "Finished with RF-gpt - Time elapsed: 6.69\n",
      "\n",
      "Finished with SVM-gpt - Time elapsed: 7.35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "for X_train, X_name in zip([tfidf_train, w2v_train, bert_train, gpt_train], ['tfidf', 'w2v', 'bert', 'gpt']):\n",
    "\n",
    "    # recurrent neural network\n",
    "    rnn = RnnTextClassifier(\n",
    "        input_size = X_train.shape[1], output_size = len(y_train.unique()), \n",
    "        hidden_size = 256, num_layers = 2\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr = 0.001)\n",
    "    n_epochs = range(100)\n",
    "    dataset = RnnDataset(X_train, y_train_tensor)\n",
    "    data_loader = DataLoader(dataset, batch_size = int(X_train.shape[0] / 128), shuffle = True)\n",
    "    trained_rnn = train_rnn(rnn, data_loader, criterion, optimizer, n_epochs)\n",
    "    torch.save(trained_rnn, f'models/rnn_model_{X_name}.pth')\n",
    "    print(f'Finished with RNN-{X_name} - Time elapsed: {(time.perf_counter()-start_time)/60:.2f}\\n')\n",
    "\n",
    "    # logistic regression\n",
    "    lr = LogisticRegression(max_iter = 1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    joblib.dump(lr, f'models/lr_model_{X_name}.joblib')\n",
    "    print(f'Finished with LR-{X_name} - Time elapsed: {(time.perf_counter()-start_time)/60:.2f}\\n')\n",
    "\n",
    "    # random forest\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "    joblib.dump(rf, f'models/rf_model_{X_name}.joblib')\n",
    "    print(f'Finished with RF-{X_name} - Time elapsed: {(time.perf_counter()-start_time)/60:.2f}\\n')\n",
    "        \n",
    "    # support vector machine\n",
    "    svm = SVC()\n",
    "    svm.fit(X_train, y_train)\n",
    "    joblib.dump(svm, f'models/svm_model_{X_name}.joblib')\n",
    "    print(f'Finished with SVM-{X_name} - Time elapsed: {(time.perf_counter()-start_time)/60:.2f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuny",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
